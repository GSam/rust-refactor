\section{Background}\label{C:back} 

To provide the necessary context for understanding the Rust specific terminology and concepts, this section explores some basic concepts within Rust with the help of the official Rust documentation \cite{doc15}. A vital area for this work to highlight is the Rust compiler, known as rustc, which forms most of the enabling infrastructure that allows refactoring to occur. 

To introduce the syntax of the Rust programming language, Figure \ref{Fig:hello} annotates a simple Hello World Rust program with a few additions. Notice how variables have to be annotated with `mut' in order to mutated, and how this applies to any references.

\begin{figure}[h]
\centering
\begin{verbatim}
// This is a comment
// You can have `use' statements to import items
// into the namespace
use std::collections;
use std::*;

// This is the main function
fn main() {
  let a = 2; // variable declaration
  let mut b = 3; // mutable variable declaration
  let c = &a; // 'borrow' or reference to a
  let d = &mut b; // mutable 'borrow' or reference to b

  // Print out text:
  // Note: println! is actually macro with
  // the exclamation mark.
  println!("HELLO WORLD!");
}
\end{verbatim}
\caption{Hello World example}
\label{Fig:hello}
\end{figure}

\subsection{Language features}

\paragraph{Ownership}
In Rust, variable bindings have `ownership' of whatever they are bound to \cite{docowner15}\cite{rustbook15}. When an owning variable moves out of scope, any resources (including those on the heap) can be freed and manual cleanup is unnecessary. By default, Rust has `move semantics' where by assigning the value of one variable to another, the object is moved to the new variable and the old variable cannot be used to reference this object anymore. Alternatively, there is the option of performing cloning and the ability to default to copying behaviour, much like primitives within the Rust language (e.g. u32 or i32 for 32-bit signed/unsigned integers).

\begin{figure}[h]
\centering
\begin{verbatim}
let v = vec![1, 2];
let new_location = v;

println!("This line causes a compiler error {}", v[0]);
\end{verbatim}
\caption{Ownership error: Use of a moved value \emph{v}}
\end{figure}

While straightforward ownership with cloning probably provides a good deal of the necessary functionality for general purpose programming, it would likely be tedious \cite{docowner15}. For instance, passing in a variable to a function would simply `eat' the variable and it would have to be part of the return so that the original variable can regain ownership (fortunately, Rust supports a return tuple). Rust supports references and borrowing, where an object can be borrowed instead of being owned. Borrows must conform to certain constraints: either there are only read-only borrows, or there is a single mutable borrow \cite{docborrow15}. Borrows must also be nested correctly so that a borrow does not exceed the lifetime of (the borrow of its) owner. By adding these restrictions, memory can usually only be modified by one place and it allows compile-time abstractions (without runtime penalty) to ensure memory safety. On the other hand, there is also the concept of interior mutability, which is an attempt to follow ownership rules at runtime. While a binding (and associated memory) might appear superficially to be immutable, it may actually be modified. This idea is useful for adapting implementations, enabling flexibility for concepts such as interior caching and essentially reducing the burden of too much cloning.

\paragraph{Annotating lifetimes}
This system does potentially add complexity during coding; when returning references for instance, the compiler might need additional help to infer the borrowing lifetimes of the different parameters and returns. In order to do so, much like generics (in Java or C++) which parameterize functions or types over some generic type, there is the additional of lifetime parameters (which precede any normal generic types and annotated using a prefixing apostrophe) to describe the lifetime of the input and output variables.

%[TODO] Example of lifetime parameters?
\begin{figure}[h]
\centering
{\verb|fn foo<|}{\color{blue} \verb|'a|}{\verb|>(x: &|}
{\color{blue} \verb|'a|}{\verb| Debug)|}
\caption{Function declaration: Lifetime parameter \emph{'a} marked in blue}
\end{figure}

For the purpose of this work, most of what needs to be understood is that lifetime parameters act as part of the type information of a variable (or signature). They denote information about a particular `borrow' and are used by the compiler to determine whether or not the ownership rules have been followed. Once compiled, they don't serve any real purpose.

In most cases, descriptions of lifetime parameters follow quite simple patterns, and so Rust introduced the concept of lifetime elisions to reduce code verbosity. Elisions allow omission of lifetime parameters (based on some rules), requiring them to be specified only when they are actually required (by compiler inference). Based on a survey, the elision rules covered around 87\% of uses in the standard library \cite{elisionrules}. Lifetimes appear in either the input or output positions, again, denoted by a leading apostrophe. For functions, input positions refer to the types for the formal arguments, and the output refers to the result type. The rules are described as follows: Each elided lifetime in input position becomes a distinct lifetime parameter. If there is exactly one input lifetime position (elided or not), that lifetime is assigned to all elided output lifetimes. If there are multiple input lifetime positions, but one of them is \&self or \&mut self, the lifetime is assigned to all elided output lifetimes. Otherwise, it is an error to elide an output lifetime.  Reification of lifetimes refers to performing the opposite of elision, i.e. reintroducing a lifetime parameter where one did not exist previously. Although this all appears very abstract, Section~\ref{C:wd} should better clarify the concepts: the built tool performs elision and reification transformations to source code.

%[this is pretty hard to explain without spending pages and pages...]
\vspace{5mm}
Elision looks something like:
\begin{verbatim}
fn get_mut<'a>(&'a mut self) -> &'a mut T
ELIDED: fn get_mut(&mut self) -> &mut T
\end{verbatim}

While reification looks something like:
\begin{verbatim}
fn get_mut(&mut self) -> &mut T
REIFIED: fn get_mut<'a>(&'a mut self) -> &'a mut T
\end{verbatim}

\paragraph{Traits}
A trait is a collection of methods defined for an unknown type which is denoted by `Self'. Traits define interfaces, but do not follow the standard notion of inheritance \cite{traitexample15}. Traits can be composed of other traits, and any conflicting methods must be resolved if they occur. Traits allow definition of default implementations and most importantly, traits are implemented for data types -- specifically structs and enums. Basically, where a type implements a trait, you can be expected to call any of the methods present in that trait on any object of that type. Rust allows limited operator overloading using traits. The `+' operator for instance, can be overloaded by creating an implementation for the corresponding trait `Add'.

\begin{figure}[h]
\centering
\begin{verbatim}
  struct Cow;                  trait Moo {
  impl Moo for Cow {                 fn moo(&self);    
    fn moo(&self) {            }
      println!("Moo");                     
    }
  }
\end{verbatim}
\caption{Simple trait example}
\end{figure}

\paragraph{Modules}
Modules provide a logical unit of code organization that can be used to hierarchically divide code while managing visibility \cite{docmod15}. The appearance of the `use' declaration allows binding of a module path to a shorter name and utilization of code stored within such a module namespace. Modules can be nested within the same file, created as a file with the name of the module or as a subdirectory with the name of the module.

% Macro system?
% Redeclaring let bindings?
% Error handling (Option<>, Result<>, try!, panic())

\subsection{The Rust compiler}
As one of the most prominent examples of Rust code, the self-hosted Rust compiler, rustc, has on the order of hundreds of thousands of lines of code \cite{openhub15}. Due to the fact that it is written in Rust, the build (or bootstrapping) process of rustc occurs in multiple stages (with prior stages of the compiler compiling newer stages) and requires the download of a compatible snapshot of the Rust compiler (for stage 0) in order to compile correctly \cite{makefile15}. In order to generate machine code, Rust relies on an LLVM backend and their associated LLVM IR (or intermediate representation).

In Rust, a `crate' forms a compilation unit \cite{examplecrates15}. To compile a crate, a crate root Rust file is supplied to rustc, which merges the contents of referenced modules before the actual compiler is run over it. As a consequence, modules are not compiled individually, only crates. In order to link an external crate, the `extern crate' declaration must be used. This will both link the library and import all items under a module named the same as the library. 

\paragraph{Compilation stages}
In the Rust compiler, the compilation of a Rust program is divided up into a number of stages \cite{driver15}. A high level overview is as follows:

\begin{enumerate}
\item Parse input -- From the crate entry point, begin building the AST and parsing all the associated files.
%\itemsep0em 
\item Configure and expand -- Early phases of the compiler, including loading plugins and dependencies, and macro expansion.
%\itemsep0em 
\item Analysis -- Performs resolution, type checking and other analysis checks on the crate.
%\itemsep0em 
\item LLVM translation -- Translate the analysis and AST to an input form for LLVM.
%\itemsep0em 
\item Run LLVM -- Run LLVM to produce a bitcode, assembly or object file as a result.
%\itemsep0em 
\item Link LLVM output -- Run the linker to produce a finished executable or library.
%\itemsep0em 
\end{enumerate}

\paragraph{Save-analysis}
A major feature provided by the compiler to enable refactoring, is the ability to produce a summarized analysis after Stage 3 of compilation. This output in the form of a comma separated (csv) file and collates a number of useful artifacts for annotating source code and performing code analysis. In order for rustc to generate this output, a flag must be supplied to the command line `{\verb|-Zsave-analysis|}'. Besides the refactoring tool produced as part of this work, the save-analysis output is also used by a Rust plugin for DXR, which is a source code browser developed and used frequently by Mozilla \cite{dxr15}.  

% https://github.com/nrc/dxr/blob/rust5/dxr/plugins/rust/__init__.py
% DXR_RUST_TEMP_FOLDER
% Output dir can also specify csv folder, ./XXX/dxr or ./dxr-temp

\paragraph{Analysis of the csv output}
In order to provide context to the nature of the output, a brief summary is presented with variables in Figure \ref{Fig:csv}.

\begin{figure*}
\vspace{5mm}
\noindent
Variable declaration:
\begin{verbatim}
variable,file_name,"basic_rename.rs",file_line,3,file_col,8,extent_start,38,
extent_start_bytes,38,file_line_end,3,file_col_end,9,extent_end,39,
extent_end_bytes,39,id,"13",name,"y",qualname,"y$13",value,"y = 20",
type,"i32",scopeid,"0"
\end{verbatim}

\noindent
Variable reference:
\begin{verbatim}
var_ref,file_name,"basic_rename.rs",file_line,10,file_col,20,extent_start,
178,extent_start_bytes,178,file_line_end,10,file_col_end,21,extent_end,
179,extent_end_bytes,179,refid,"13",refidcrate,"0",qualname,"",scopeid,"4"
\end{verbatim}

\caption{Example csv output from \emph{-Zsave-analysis}}
\label{Fig:csv}
\end{figure*}

At the top of the csv there is metadata associated with the crate for which the analysis was generated. Following crate definitions, there are lines of the output which correspond individually to declarations and references for variables, types, functions etc. The first value of each of these lines is the type of information described by the line. In the cases described by Figure \ref{Fig:csv}, they are {\verb|variable|} and {\verb|var_ref|} for variable declaration and variable reference respectively. The remaining attributes are mostly shared between the different types of declarations and references. File name indicates the file in which an item is declared or referenced. The next attributes ({\verb|file_line|}, {\verb|file_col|}, {\verb|extent_X|}) concern where in the source file the code in question appears in terms of file lines, file columns, byte indices relative to the file and byte indices relative to the entire crate. In variable, there is the {\verb|id|} attribute which corresponds to the node id of the corresponding node in the abstract syntax tree built by the compiler. In {\verb|var_ref|}, this attribute is replaced by {\verb|refid|} which corresponds to the node id which the reference is referencing. Here they both refer to "13", which indicates they concern the same variable. In static situations, like variable usages, the {\verb|refid|} corresponds directly to the associated node for the declaration. However, this is not always the case with method calls and dynamic dispatch. In these cases, it is necessary to refer to an additional attribute called {\verb|declid|}. With types, the node id may be aliased with the node id of the constructor instead of the node id of the declaration of the type, and this must be resolved within the refactoring program.

\paragraph{Name resolution}
In Rust, a path (within the compiler) corresponds to the concept of a name formed by a sequence of individual identifiers and other supporting information, e.g. std::cmp::PartialEq \cite{docpath15}. Each of these segments might have type or lifetime information, but more importantly, they have a name and a syntax context. The names are stored, or interned, in an interner table and the syntax context provides information to specify where this name might be located. The appearance of a syntax context is due to Rust macros, which allow usage of variables and variable names introduced in a macro without conflicting or aliasing against any names once the macro is expanded or inlined. This behaviour is only part of the functionality provided by Rust macros, which allows manipulation of the AST and more powerful and (type) safe transformations than text replacing macros \cite{keep15}.

In many ways, the functionality of Rust macros parallels the functionality aiming to be achieved by refactoring, particularly in the case of renaming. However, the goal of each is divergent enough that the two systems are not compatible. One of the goals of hygienic macros which work with the compiler, is not to introduce a name that will conflict with any existing names \cite{keep15}. This is done with the use of a syntax context, but the (textual) name itself does not actually change. Essentially, it can produce an AST which would not compile within the Rust compiler, however, when converted to source text where the differences in syntax contexts are unavailable, the code could compile just fine. In Figure \ref{Fig:striping}, we can see how this can be visualized by striping code with colours. The `a' in red might refer to {\verb|a#24|} while the `a' in blue might refer to {\verb|a#30|}, placing each in different syntax contexts. Renamings performed on source text need to understand where the textual names conflict, and reusing a macro based system as currently implemented would only provide any significances internally in the compiler. Extensions to the macro system could be possible and parts of the system made to the advantage of refactoring, however, it appears fundamentally they are not compatible enough for one to be used as a replacement for the other.

\begin{figure}
{\color{blue}
\verb|                        let four = {|
}

{\color{red}

\verb|                            let a = 42i;|
}

{
\color{blue}

\verb|                            a / 10|
}

{\color{red}

\verb|                        };|
}

\caption{Example of syntax contexts and identifier striping}
\label{Fig:striping}
\end{figure}

Paths form an important part of name resolution within the compiler. Forming the first part of the analysis phase in the compiler, name resolution walks the entire AST and attempts to check that the different paths exist at the contexts in which they were used or their lexical scope \cite{driver15}. In the case of refactoring, this is an important piece of functionality which needs to be used: First to ensure that a proposed name for a refactoring (or renaming) doesn't already exist and will not conflict, and secondly, that once any references to a name are changed, they do not refer to a completely different name. This is despite potentially being spelt the same and the consequence of different scoping or shadowing rules.

\subsection{The background of refactoring}\label{S:refactorback}
Refactoring is critical part of a programmer's life in ensuring that software remains maintainable and enables future extension. Martin Fowler's definition in 1999 \cite{fowler99} defines refactoring as the following: \emph{``Refactoring is the process of changing a software system in such a way that it does not alter the external behaviour of the code yet improves its internal structure.''}

When initiating a first prototype, there is often little regard for underlying structure or overall design and in many cases, these prototypes live on beyond their original intention \cite{foote1997big}. In order to fight back against poorly structured and code which has degraded over time, a good strategy is to refactor relentlessly. In many cases, the problem context might have changed over time or a better approach has been identified. Being able to modify the code to restructure it prevents the cost of rewriting from scratch; however care must be taken to ensure that the modifications preserve the original behaviour as much as possible.

Refactoring can provide a large number of benefits. It allows improving the design of code, and reversal of code decay which occurs by addressing short term goals and failing to adhere to an overall structure. It allows better understandability of code, allowing better obedience of convention and conversions to well-known idioms. Refactoring encourages code reuse by encouraging extraction, generalization and identification of duplicate pieces of code. By improving design and testing assumptions, refactoring allows programmers to identify bugs in their programs and in general, improves productivity, allowing developers to work together better.

\paragraph{Behaviour preservation}
Bill Opdyke in Refactoring Object-Orientated Frameworks defined behaviour preservation in terms of seven properties \cite{opdyke1992refactoring}. Although taken from a C++ perspective, the definition continues to be used more widely \cite{schafer2010specification}.

\begin{enumerate}
\item Unique superclass -- After refactoring, a class should have at most one direct superclass, which is not one of its subclasses.
%\itemsep0em 
\item Distinct class names -- After refactoring, each class name should be unique.
%\itemsep0em 
\item Distinct member names --  After refactoring, all member variables and functions within a class have distinct names.
%\itemsep0em 
\item Inherited member variables not redefined -- After refactoring, an inherited member variable from a superclass is not redefined in any subclass.
%\itemsep0em 
\item Compatible signature in member function redefinition -- After refactoring, if a member function in a superclass is redefined in a subclass, the two function signatures must match.
%\itemsep0em 
\item Type safe assignments -- After refactoring, the type of each expression assigned to a variable must be an instance of the variable's defined type.
%\itemsep0em 
\item Semantically equivalent references and operations
%\itemsep0em 
\end{enumerate}

The first six properties can be verified by a single run of the compiler correctly succeeding; however the seventh property cannot be ensured with the same action. Essentially, the last property is to ensure that two runs of a program with the same inputs will always produce the exact same outputs as the original, unchanged program. There are a number of very simple cases where compilation may succeed while the functionality of a program has changed, one major cause of which is shadowing (explored in detail in Section~\ref{C:wd}). 

\begin{figure}
\begin{verbatim}
  let a = 2;         let b = 2;
  let b = 4;         let b = 4;
  let c = a + b;     let c = b + b;
\end{verbatim}
\caption{Renaming local variable \emph{a} to \emph{b}}
\label{Fig:opdyke}
\end{figure}

Figure \ref{Fig:opdyke} describes an example in Rust. In the original code {\verb|c|} evaluates to 6, but in the renamed case, after renaming {\verb|a|} to {\verb|b|}, {\verb|c|} evaluates to 8 due to shadowing. The situation here is slightly unique with local variables since Rust allows duplicated variable bindings under the same name, unlike other languages like Java. Such examples (of shadowing) are particularly worrying from a programmer's perspective since compilation failures clearly indicate issues but here successful compilation would provide no indication of preserved behaviour. The program would simply function differently without the programmer necessarily noticing any changes. Although this might be caught with testing, having such a comprehensive test suite for every such eventuality is impractical.

\paragraph{Fowler's take on refactoring}
The book Refactoring: Improving the Design of Existing Code by Fowler et al. \cite{fowler99}, collates a catalog of more than 70 refactorings and although written more than 10 years ago, it remains a common end-goal for those attempting to implement a comprehensive refactoring tool \cite{jetbrains15}. One of the first key ideas that the book asserts is that before refactoring occurs, a solid suite of tests, particularly unit tests should be present to ensure that functionality is never modified. The book uses bad code smells such as code duplication, long methods and large classes, as rationale for the various refactorings and when they should be performed. 

The refactorings described generally fall under the following categories:
\begin{itemize}
\item Composing methods -- Grouping code into methods, extraction of code fragments, inlining temporary variables
%\itemsep0em 
\item Moving features between objects -- Changing the target of a method, moving fields
%\itemsep0em 
\item Organizing data -- Modifying type, abstraction or data representation
%\itemsep0em 
\item Simplifying conditionals -- Divide or join conditional expressions, remove control flags
%\itemsep0em 
\item Simplifying methods -- Renaming methods, adding or removing parameters
%\itemsep0em 
\item Generalization -- Moving fields or methods through an inheritance hierarchy
%\itemsep0em 
\end{itemize}

The book describes three key details in providing a practical refactoring tool, using the Smalltalk Refactoring Browser -- one of the earliest and most comprehensive refactoring tools -- as a guideline. The first is speed: If it takes too long, a programmer would likely prefer to just perform a refactoring by hand and accept the potential for error. The second is the ability to undo: Refactoring should be exploratory, incremental and reversible assuming it is behaviour preserving. The last is tool integration: With an IDE that groups all the tools necessary for editing, compilation, linking, debugging etc., development is more seamless and reduces the friction in adopting additional tools into a workflow. 

% Speed. Undo. IDE browser. Smalltalk refactoring browser. 

%%%%Two hats, adding function and refactoring.

\paragraph{Precondition based refactoring}
While defining behaviour preservation in his thesis, Opdyke also details a precondition based approach to performing refactorings \cite{opdyke1992refactoring}. In his work, he examines a number of refactorings individually and outlines a number of preconditions which must apply for each to ensure their correctness. Although not fully comprehensive, the descriptions cover enough detail to compare against and the scope of the refactorings covered easily covers the scope of this work. The approach helps to form the basis of inline-local and reification or elision of lifetimes which have notable Rust specific context, detailed in Section~\ref{C:wd}. 

%\subsection{Examples of refactoring tools}\label{S:exback}
%
%\subsubsection{Java refactoring}
%
%\paragraph{Overview}
%In Java, refactoring tools generally rely upon the use of compilation units which are ordinarily defined as source files. Refactoring with Java generally consists of a number of steps. The first is determining a set of affected compilation units, using a search engine or database. Secondly, the Java Model API allows implementation of preconditions like the presence or existence of some member. Third, manipulation of the abstract syntax tree and then finally code generation \cite{baumer2001integrating}. 
%
%\paragraph{Eclipse IDE}
%The Eclipse open source platform provides an IDE with a number of refactorings available for Java (and a few other programming languages) \cite{widmer07}. A list of these refactorings is presented in the Appendix in Figure \ref{Fig:eclipse}. Beyond just providing editing and refactoring capability, Eclipse provides the Java Development Tooling platform (JDT) with API for programmers to build their own refactorings. Eclipse supports safe renaming and move refactoring as well as more complex refactoring like extraction or inling of methods or variables. Eclipse supports a number of inheritance based object orientated refactorings like allowing users to replace occurrences of a type with one of its supertypes if possible. Eclipse also allows restructuring at the file structure level, allowing renaming of source code files and classes. Eclipse provides the ability to build refactoring scripts and has history functionality for recording when and where a refactoring has occurred in a project. 
%
%\paragraph{IntelliJ IDEA}
%Built primarily as a Java IDE, IntelliJ IDEA supports a number of refactorings for Java and provides support for other languages with the addition of plugins \cite{jetbrains15}. IntelliJ IDEA was one of the first to cover a good deal of the refactorings described by Fowler in Java. In terms of current support, the refactorings have a significant crossover with Eclipse however, implementation for a particular refactoring does not necessarily mean equal support or similar limitations. Beyond simple renaming, IntelliJ, ahead of Eclipse, has attempted to address the issue of some refactorings which would preserve behaviour with only minor automated modifications \cite{schafer2010specification}. An example of this behaviour is the automated qualification of fields in a constructor as described by Figure \ref{Fig:qual}. Producing a behaviour modifying shadow of the field {\verb|x|}, the renaming could be outright prevented; however, qualification by `this' is a common pattern seen in Java code. One of the goals of refactoring is to ensure idiomatic code and given the compiler and language can provide the necessary checks; such a refactoring should be desirable. Our tool, like others, must consider these trade-offs.
%
%\begin{center}
%\begin{figure}
%\begin{verbatim}
%class Example {        class Example {
%    int x;               int x;
%    Example(int y) {     Example(int x) {
%        x = y;             x = x; // Ideally: this.x = x;
%    }                    }
%}                      }
%\end{verbatim}
%\caption{Renaming constructor parameter \emph{y} to \emph{x}}
%\label{Fig:qual}
%\end{figure}
%\end{center}
%
%Another example is the automated import of packages when refactoring, to necessarily qualify previously unqualified identifiers. By stepping beyond the guaranteed subset of refactorings which would provide no conflicts, IntelliJ runs the risk of introducing cases which should not work. Such cases have been identified by other literature and although these failures might only be valid for older versions and errors may have been fixed, it is likely there are a number of other edge cases \cite{jemerov2008implementing}. IntelliJ IDEA maintains a large suite of tests to ensure correctness of renamings, however beyond tested scenarios; edge cases may not be correctly handled and are still allowed -- although a post-refactoring compilation could identify many of these cases. Generating the necessary rules for Java is quite complex, but more importantly, quite language specific. IntelliJ developers have previously noted the difficulty of creating a common infrastructure for refactorings over different languages \cite{jemerov2008implementing}.
%
%IntelliJ offers a `smart' replacement feature which the tool describes as `structural search and replace' \cite{jemerov2008implementing}. In the context of extremely large codebases, the cost of compilation might be extremely high and prevent in-depth code analysis. By allowing a partially safe restructuring mechanism, more flexibility is given to the programmer and allows definition of custom refactoring operations. Furthermore, it would help prevent programmers from performing potentially unsafe blind search and replace directly. On the other hand, the feature has been cited as hard to use, requiring a custom query language and perhaps lacking in comprehensiveness \cite{jemerov2008implementing}.
%
%\paragraph{JRRT and aspect orientation}
%Taking an approach different to those previously taken in Java, JastAdd Refactoring Tools (JRRT) attempts to perform refactoring with the use of aspect orientation \cite{schafer2010specification}. Testing against other refactoring tools for Java (although this was in 2010), they found that a number of other refactoring tools, including Eclipse, failed to account for tricky edge cases and precondition based refactoring either imposed too great of a restriction on refactorings or they were too weak to preserve Opdyke's 7th constraint. JRRT incorporates parts of aspect orientated programming in order to achieve a number of refactorings forming most of the core of the Eclipse tools they built upon. JRRT uses static aspects, instead of dynamic aspects allowing instrumentation of code by crosscutting behaviour within these aspects. Aspects form modular units of behaviour changes and define inter-type declarations which correspond to members of existing types and contexts. After the necessary modifications are made to the AST, the aspects are then converted to source code.
%
%In a dissertation documenting JRRT by Max Schafer, an author of the JRRT, he comments on the difficulty of defining the terms program behaviour and behaviour preservation. The definition should be realistic, not just an idealistic approximation; it should be precise and define exactly what it means to preserve behaviour; and it should be general enough that refactorings typically considered behaviour preserving can also be found under this new definition. Opdyke's definition is appealing due to its generality and application to languages beyond which it was defined, but the definition lacks precision. In Java specifically, changes considered behaviour preserving by Opdyke, like adding an unused field, might modify behaviour in a distributed application setting where serialization affects memory layout and IO behaviour. Schafer notes the lack of progression on a more concrete definition since Opdyke and takes the more practical approach, much like existing work by Roberts \cite[p. 111]{schafer2010specification}, of defining behaviour preservation in terms of individual refactorings. A large number of work focuses only on test suites passing and refactoring being correct on a subset of programs. Formalization appears to have made little headway in this area, with work only dealing with unrealistic subsets of languages, like Java, which do not address any of the significant complexities that are associated with refactoring.
%
%\subsubsection{Functional programming refactoring tools}
%
%The limitations of current formalization appear to extend to the world of functional programming as well, with verification only done under small subsets of a language or only informal arguments of correctness being given (which is predominantly what has been done here). With Brown of the Haskell Refactoring project, we can note comments on the localized nature of implemented transformations despite the opposite trend in general refactoring \cite{brown2008tool}. The Haskell refactoring tool (HaRe) project is an example of a refactoring tool built for a functional programming environment. Described in the PhD thesis of Brown \cite{brown2008tool}, it was found that Haskell mostly helped facilitate simple, atomic refactorings, and based on a case study, composition of these atomic refactorings was more useful than standalone complex refactoring. Haskell in particular was found to be quite difficult to refactor with many complexities in the language: type signatures, pattern matching, guards, where clauses and type classes. In many cases, complex refactorings posed intriguing questions as to the intended behaviour of a refactoring and imposed questionable default behavour \cite{sculthorpe}. In many cases, only a tractable subset of possible cases was treated and only a partial solution supplied. Not knowing what to do with multiple possible options or what to offer to a user is a general problem, being a point of discussion in Section~\ref{C:future}. Brown found that there were three main refactorings which were commonly used in Haskell: 
%
%\begin{enumerate}
%\item Generalize definition -- Selects an expression to generalize to a function parameter
%\item Introduce new definition -- Select code to be expressed as a new function
%\item Folding -- Replacing all expressions which can be expressed with a given function
%\end{enumerate}
%
%Folding in particular requires duplicate code detection, and existing work takes various approaches, at the text level, at the token level and at the AST level. HaRe deals with duplicated AST which is generally more costly than the other approaches. However with AST representation, individual names may be discarded and more sophisticated detection of clones with semantic analysis may be applied. 
%
%\subsubsection{The Scala refactoring tool}
%
%Built as part of the master's thesis for Mirko Stocker \cite{stocker2010scala}, the Scala refactoring tool forms one of the major refactoring tools for the Scala programming language. According to the homepage scala-refactoring.org, the currently supported refactorings are renaming, extract local, inline local, extract method, organize imports and tentatively move class \cite{scala15}. The major focus of the master's thesis was the extract method refactoring, a non-trivial refactoring which allowed selection of a number of expressions to be replaced with a new method with the replaced expressions appearing in the definition.
%
%\paragraph{Overview of the approach}
%In this tool, the refactorings are built around a core library which is built around manipulating the AST. Analyzing the AST, the tool builds a number of indexes, including a global index in order to identify corresponding occurrences of symbols in the AST. In this way the tool can identify both declarations and usages of different items, and do so as efficiency as possible using indexing. In Sections~\ref{C:eval} and \ref{C:future}, we will see more on why runtime speed and modularity considerations are important. By manipulating the AST, the original source code can be manipulated arbitrarily and accurately in a way which will always continue to compile correctly. But notably, this relies on the ability for pretty-printing based on an AST.  Like Java, source files act as compilation units, allowing incremental compilation of programs.
%
%\paragraph{Limitations}
%One of the major limitations of working solely with the AST is that the correspondence to the source code is somewhat lost in the process. Although pretty printing can occur, syntactic sugar within the language means that the resulting code may be significantly different to the original code and modify much more than the refactoring desired. Furthermore, certain constructs defined entirely in syntactic sugar cannot undergo refactoring due to their absence in the AST. The author does actively attempt to remedy this problem by applying an algorithm to line up the output code to the original source based on certain landmarks present in the text. However, this is not a trivial operation and serves as significant complexity; being an operation which may not always succeed. This problem is also one of particular interest to Rust, mostly because macros are effectively syntactic sugar.
%
%\subsubsection{The Go programming language}
%
%\paragraph{gofmt tool}
%The gofmt tool was originally designed to pretty-print the AST for Go source code. This allowed developers to much more easily follow coding convention by using an automated tool to adjust their code. The gofmt tool was extended to allow a limited, but flexible refactoring capability \cite{gofmt15}. A similar rustfmt tool exists for pretty-printing Rust code \cite{rfmt}, obviously lacking any refactoring capability, and this tool was used during this project to help style any Rust code.
%
%Bearing similarity to IntelliJ's `structural search and replace', the tool functions by parsing two Go compatible expressions as the input expression and the output expression. Any expressions which match the input expression undergo the transformation into the corresponding output expression. This is more powerful than a simple text-replace because the expressions have to match the corresponding AST nodes and there is no need to account for other irrelevant details such as whitespace. Whitespace in particular can be solved with a text replace, but requires more complicated regular expressions to match all the possible occurrences. This functionality of gofmt allowed the Go team to build the gofix tool which allowed Go user-code to be fixed during the earlier stages of the language when the API had not been set entirely in stone yet \cite{gofix11}.
%
%\begin{quote}
%``Gofix has already made itself indispensable. In particular, the recent reflect changes would have been unpalatable without automated conversion, and the reflect API badly needed to be redone. Gofix gives us the ability to fix mistakes or completely rethink package APIs without worrying about the cost of converting existing code.'' -- Russ Cox, Google Developer \cite{gofix11}
%\end{quote}
%
%\paragraph{gorename tool}
%Although gofmt allows some checking of compatible transformations, it cannot always ensure that the resulting transformations would still compile or that new identifiers would not cause conflicts. Recently in 2014, the gorename tool has been created which allows the type-safe renaming of most Go identifiers, whether they are type names or variables for instance \cite{gorename15}. One of the key observations that can be made from inspecting the code is that a form of name resolution is used to identify potential renaming conflicts. In this manner, it does not appear to be the case that full compilation is necessary to check the validity of renamings. This ideal has influenced implementation decisions for our tool, as well as evaluation criteria featured in Sections~\ref{C:impl} and \ref{C:eval}.
%
%Comparing gorename to the gofmt tool, gofmt has the advantage of being more flexible by allowing functionality beyond renaming. By being able to manipulate the AST, gofmt helps to better facilitate a number of the various refactorings highlighted by Fowler and in this way, the two tools are not competing, but complementary.