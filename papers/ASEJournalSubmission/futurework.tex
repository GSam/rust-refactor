\section{Discussion}\label{C:future}

The work done so far serves as definite proof of the concept and the approaches used to build the refactoring tool. As explained by the evaluation, there is still work to be done to allow users to make the most out of the tool; both on this tool and more widely.

% The scope of this additional work extends far beyond the time allocated and should serve as an adequate starting point for those looking to extend this work in the future. At this point the goal of this work is to address immediate shortcomings in the tool and to leave the more complicated refactoring and infrastructure changes for others (while providing useful commentary and insight). Lastly, with Rust being an open source project, there is the issue of upstreaming code and being mindful of the fact that the tool should follow as many coding conventions as possible.

% DON'T FORGET to mention
% Contribution is about a dozen commits to the main compiler repository: https://github.com/rust-lang/rust/commits/master?author=gsam

\subsection{Future work on this tool}
\subsubsection{Wider project interaction}
% !!!!!!!!! TO INSERT !!!!!!!!!!!!!!!!!!!As of right now, allowing additional dependencies is managed by an environment variable which is neither robust or `safe' (due to the fact that changing this across threads for instance will likely cause negative consequences). Modifying the library API so that the calls have the additional dependencies handed to them appears desirable, however, slightly involved in any case.

One area of particular interest would be to investigate the overall effect of refactorings across multiple crates and how warnings (or patching scripts) should be presented when changes occur that might affect a public API. Examining the evaluation, this appears to be a major shortcoming in how not only this tool functions, but how other tools function across codebases. An investigation of Crates.io \cite{cratesio15}, the Rust package repository might yield interesting findings on crate interaction. One goal might be to describe the best steps to take when an API must change and how that should be dealt with from a community perspective. 

%Looking at how the UNIX/Linux community deals with API or ABI changes in their open source software should also prove useful.

\subsubsection{Testing and current test failures}
Going forward, the refactoring tool needs more testing to ensure that the current set of refactorings are correct. In adding functionality, tests ensure that existing refactorings are not broken with further changes to the tool. Furthermore, the refactoring tool evolves independently of the Rust compiler and changes to the compiler inevitably affect the outcome of the tool. Already, a number of existing compiler bugs have been found and reintroducing them by accident does not appear to be difficult with current limitations in the amount of testing and documentation available for some areas of the compiler. In this manner, expanding testing does not necessarily need to be confined to the refactoring tool. 

% At the moment, it has much less testing for multi-file. 

With limited experience with Rust in general, using real-world code to attempt refactorings would help test for more obscure corner cases which may have been overlooked. Some initial testing with Crates.io \cite{cratesio15} has been done along with initial analysis of its efficiency and usefulness, but more should be done in the future. 

%Furthermore, additional analysis of the efficiency of the tool can be produced and speculations made in regards to its usefulness. 

As of right now, the test cases identify roughly 10 incorrectly handled, but somewhat exceptional cases. A few of them likely require the addition of further investigation to determine their underlying source of error. On the other hand, a few of them are relatively subjective, in that they could be solved by simply restricting the inputs to the tool. One clear issue is that function local definitions currently cause name resolution issues. They appear to be at least solvable within the current tool architecture though. Elision is another source of failures, being only partly complete, having holes which were noted in Chapter \ref{C:impl}.
%  For instance, crate relative names break their current qualifications (further explanation?).  Furthermore, xxx.

\subsubsection{Future refactorings}
In terms of future refactorings, there are a number of next-step refactorings which should use most of the same infrastructure of the existing tool. For instance, trait renaming should use most of the same conventions provided by concrete types like structs or enums. This is because of the output of save-analysis and the simplifications made in process of generating the csv output. Renaming and modifying type aliases should present interesting problems due to deviations from typical object-orientated languages, along with type parameters and associated types. Furthermore, as previously described, renaming types declared within some function introduces issues to do with path resolution that still need to be resolved. To do so requires determining which parts of the path are `private' to that scope and should not be used in the general case. The treatment of types in Rust generally complicates matters, and this is already with the simplifications made from save-analysis.

In the context of Rust, there are a number of refactorings which would be more unique. Extraction of a method, as described by Fowler, presents a number of intricacies tied with lifetimes and the ownership system in Rust. Ensuring borrows are made correctly (or move semantics if a constructor is present in the extracted code) presents difficulties specific to Rust refactoring. While other refactoring changes are semantics preserving, extraction of a method could introduce new lifetimes and verification of correctness may not be so trivial. Similarly, extraction of local variables appears to provide difficulties with the ownership system and should be tackled first to provide insights for approaching method extraction.

Although not implemented here, inlining of functions was considered to some depth. Rust has the fortunate advantage of being able to evaluate a block of code as an expression, returning the result of the final line. This should allow relatively straightforward inlining of function bodies when there is a single return at the end, or no return at all. Where it becomes more complicated is with early returns whose control flow cannot be modelled simply by a block expression (which do not have the concept of early returns). This could normally be mitigated by the use of a labelled goto-like construct, however, Rust does not support goto which makes this a much more difficult problem. Whether or not classic gotos can even be implemented in Rust appears to be an open problem due to the marked increase in difficulty in static analysis (scoping or typing rules). There are labelled loop break-continue statements, so one approach might be to wrap the block in a loop. Further issues include redeclaration of arguments, or alternatively renaming of arguments and identifying when double nesting of blocks is required to achieve the correct scoping and lifetimes. 

Extraction, when compared to inlining, adds additional issues such as determining code-spans for expressions or the potential use of declared but uninitialized variables. In the standard case, extracting a local appears to be feasible as long as the same scope can be achieved; however, there is yet to be any actual evidence besides conjecture.

%Such a change is common with unstable API and within Rust, although released, there are a number of unstable API. 
Converting functions to methods or vice versa is also a critical function that would be useful for the Rust community. To alleviate issues with API changes, supporting these changes with a refactoring tool would be incredibly useful, even if it only used a structural search and replace. This would provide functionality similar to gofmt and go-fix as Go originally updated their API \cite{gofix11}. Making changes which break a sizable amount of user-code is unfavourable, however, it usually must be done at some point and having a tool to remedy the stress of such a change would be good for both the developers of Rust and the community. A non-comprehensive list of further refactorings to be considered are:

\begin{multicols}{2}
\begin{itemize}
\item Inlining methods
\itemsep0em 
\item Creating or inlining modules
\itemsep0em 
\item Adding or removing function args
\itemsep0em 
\item Changing types to type aliases
\itemsep0em 
\item Extracting traits or moving inherited to trait
\end{itemize}
\end{multicols}


% List a number of refactorings which would be interesting in the context of Rust
% Comparing closures to other languages
% Extract a global
% Constructor to factory?
% Unknown the extent to which refactoring works in unsafe code blocks

\subsubsection{Modularity of code}
While the evaluation mostly concerned external qualities of the tool, another aspect that could have been considered is the relative difficulty of adding or modifying refactorings. Currently, the compiler is tightly coupled with the refactoring tool and if you compare this to the Scala refactoring tool, you might find the latter uses much more well-established API and is more readily composable. In a similar way, JRRT likely benefits from the `obliviousness' of aspects \cite{aop}, being structured in such a way that injecting code is not noticed by the original program code. Whether this should be achieved here, or by working more with the compiler, it appears an important detail to enable wider contributions to the tool.


\subsection{Limitations in the performance evaluation}
Perhaps the most obvious shortcoming in these tests is the lack of sufficient data points, especially when trying to justify a trend. Finding the current set of examples was already extremely difficult given the size of the crates, made worse by the lack of macro support. Originally, a fifth crate `bitflags' was to be analyzed; however, the entire crate formed a macro expansion, preventing any refactoring. `libc' was almost entirely header declarations, having no `let' bindings, which made it difficult to generate the full set of refactorings. Performing scaling tests on more than just renaming types would have been helpful to prove worst-case linear scaling, especially for renaming functions as it was noticeably slower. But finding any meaningful variation of occurrences was difficult, and having but a few usages was common (as seen by the concentration of points in Figure \ref{Fig:comparerefs}). The relative size of the crates is still quite small and how the tool functions on much larger codebases is still completely unknown. Missing inline definitely limits the amount of useful conclusions that can be draw here. But the fact that few suitable locations for its usage were found questions the usefulness of the refactoring and the associated checks (given how incomplete they are). 

\subsection{Subjectivity in refactoring}
One of the reasons refactoring is not a straightforward topic is because of the different ways people might interpret a refactoring. In some domains or context, a perceivable behaviour violating change may not even matter. There are some cases where garnering user feedback will offer a clear-cut decision, but there are likely cases that will not. Caring is important because even if a user dislikes the way code is pretty-printed, for instance, they might avoid a tool entirely. Offering choice sounds good, but deciding what is an error, or a warning is still not easy (with better granularity here being a goal). One recent example that has come to light is the existence of conditional compilation within Rust. If performing a refactoring breaks other platforms, but not yours, should that be a problem? In particular, you may not have the required analysis of the remaining code, in which case a failure might be better.

\subsection{Missing formalisms}
As raised throughout this report, lack of formalism is an ongoing problem in the area of refactoring. General progress in the field of refactoring appears quite slow, with many opting to simply produce implementations rather than tackling the problem at large. In terms of missing formalisms, refactoring is not the only one at blame it appears. Much in the same way, compilers encode otherwise undescribed aspects of programming languages. Rust is no different, and the case with the relatively informal elision RFC (which was descriptive but definitely not complete) was a reminder of this. Being well-defined likely would have allowed reversal of the elision rules with much more ease. This is not a reason to point blame, but to highlight the genuine scale and complexity of the projects and issues at hand. 

%\subsection{Associated areas of work or research}
\subsection{Additional compiler improvements}
% -Z save-analysis
Given the current architecture of the tool, there are a number of further improvements that could be made to the compiler to improve its efficacy and efficiency. Using {\verb|-Zsave-analysis|} provides another additional step before a refactoring can be made. Furthermore, it must be generated every time a refactoring is to be undergone. By providing a library and API with which this information can be queried using the existing compiler API, the need for csv parsing and associated unrelated complexity disappears. The actual performance offset of having to run the compiler again to generate this information does not disappear however.  

%By running all the way through to analysis, this still takes a significant amount of time. 

%[reference to compiler stages from earlier section]

%  Compiler speed-up + Name resolution at the lowest level
Helping to address performance in the compiler itself in the general case should also help to improve performance of the refactoring tool. However, based on the current design of the tool and the need for multiple runs with modified source, the issue of performance would be better addressed by improvements to the name resolution module in the Rust compiler. In particular, providing name resolution for usages instead of just at the declaration level would turn O(N) runs of the compiler into O(1) by using name resolution multiple times in a single run. Fundamentally this requires heavy changes to the structure of the compiler and the nature of name resolution. As it currently is, not every node gets its individual node id in nested expressions and to resolve this requires significantly more memory usage or novel, significant and well-architected changes, well beyond the scope of this project.

% Macro hygiene with types
% Renaming within macros - seems like a big problem

Currently macros are completely ignored by the save-analysis report generated by the compiler. As it is, this appears to be a major shortcoming in the refactoring tool that has yet to be addressed. However, not a lot can be done at this point due to the general limitations regarding macros and how they are typically ignored when generating metadata from the compiler. Without being able to identify any spans associated with macros, it isn't possible to make the necessary code transformations. Implementing the necessary functionality is certainly non-trivial and likely requires much better domain knowledge than that which could be provided here. Resolving this would also fix pretty printer abstraction layer issues.

\subsection{Miscellaneous}

%[TODO: perhaps move somewhere else]
%A user-study or fielding comments on the tool would provide some interesting feedback. In order to provide usefulness, this is incredibly important part of assessing a tool. Furthermore production of a prototype GUI tool or Emacs plugin could increase the amount of users of the tool, or people interested in the tool. That could generate interest in further development and further research began by this project.

% Macro hygiene vs refactoring
The relationship between macro hygiene and refactoring is particularly novel, but from initial analysis, does not appear to provide any particular benefits. Further research into the relationship might provide some unique insights and a system which is able to incorporate both would be of significant academic interest, and interest to this author.

Efficiency of large-scale refactoring in general needs more attention. Although analysis of individual packages on a service like Crates.io provides some benefits, it would be curious to see how a tool functions on code bases which are much larger, in the order of hundreds of thousands of lines of code or larger. Although the expectations on this particular tool are not quite so high, understanding the general tradeoffs of provably correct refactorings and time taken and developer perception of this tradeoff is likely to produce fascinating observations. Looking at the Google-scale of refactoring, Google have set up a Clang map-reduce to perform refactoring on large codebases over a network of machines \cite{carruth2011clang}. Evidently the utility and associated confidence in such refactorings led them to spend the time to create such a framework, but determining when this happens in the general case provides provocative food-for-thought.

